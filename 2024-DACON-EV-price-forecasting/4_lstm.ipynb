{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from warnings import simplefilter\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 999\n",
    "PATH = os.getcwd()\n",
    "train = pd.read_csv(f'{PATH}/data/train.csv').drop(columns=['ID'], axis=1)\n",
    "test = pd.read_csv(f'{PATH}/data/test.csv').drop(columns=['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_columns = {\n",
    "    \"제조사\": \"Manufacturer\",           \"모델\": \"Model\",\n",
    "    \"차량상태\": \"VehicleCondition\",     \"배터리용량\": \"BatteryCapacity\",\n",
    "    \"구동방식\": \"DriveType\",            \"주행거리(km)\": \"MileageKm\",\n",
    "    \"보증기간(년)\": \"WarrantyYears\",    \"사고이력\": \"AccidentHistory\",\n",
    "    \"연식(년)\": \"Year\",                 \"가격(백만원)\": \"Price\",\n",
    "}\n",
    "\n",
    "train = train.rename(columns=rename_columns)\n",
    "test = test.rename(columns=rename_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배터리 용량 결측치 처리\n",
    "upper7 = train[train['WarrantyYears']>=7]['BatteryCapacity'].mean()\n",
    "lower7 = train[train['WarrantyYears']<7]['BatteryCapacity'].mean()\n",
    "\n",
    "train.fillna(-1, inplace=True)\n",
    "test.fillna(-1, inplace=True)\n",
    "\n",
    "def fill_battery(row):\n",
    "    if row['BatteryCapacity'] == -1:\n",
    "        if row['WarrantyYears'] >= 7: return upper7\n",
    "        else: return lower7\n",
    "    return row['BatteryCapacity']\n",
    "\n",
    "train['BatteryCapacity'] = train.apply(fill_battery, axis=1)\n",
    "test['BatteryCapacity'] = test.apply(fill_battery, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = [0, 60, 80, 100]\n",
    "# labels = [\"s\", \"m\", \"l\"]\n",
    "# train[\"BatteryCapacity_cut\"] = pd.cut(train[\"BatteryCapacity\"], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "# test[\"BatteryCapacity_cut\"] = pd.cut(test[\"BatteryCapacity\"], bins=bins, labels=labels, right=False, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Year'] = 2024 - train['Year']\n",
    "test['Year'] = 2024 -  test['Year']\n",
    "\n",
    "train['WarrantyYears'] = 2024 - train['WarrantyYears']\n",
    "test['WarrantyYears'] = 2024 -  test['WarrantyYears']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_columns = ['ID', 'Price']\n",
    "categorical_columns = [col for col in train.columns if (train[col].dtype in ['object', 'category']) and (col not in without_columns)]\n",
    "numerical_columns  = [col for col in train.columns if col not in categorical_columns and (col not in without_columns)]\n",
    "\n",
    "for i in range(len(numerical_columns)):\n",
    "    for j in range(i, len(numerical_columns)):\n",
    "        train[f'{numerical_columns[i]}*{numerical_columns[j]}'] = train[numerical_columns[i]] * train[numerical_columns[j]]\n",
    "        test[f'{numerical_columns[i]}*{numerical_columns[j]}'] = test[numerical_columns[i]] * test[numerical_columns[j]]\n",
    "        \n",
    "train['BatteryCapacity/MileageKm'] = train['BatteryCapacity'] / train['MileageKm']\n",
    "test['BatteryCapacity/MileageKm'] = test['BatteryCapacity'] / test['MileageKm']\n",
    "\n",
    "numerical_columns  = [col for col in train.columns if col not in categorical_columns and (col not in without_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGRU(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.GRU = torch.nn.GRU(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size//2, hidden_size//4)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size//4, 1)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        x, _ = self.GRU(x)\n",
    "        x = self.fc1(x[:, -1, :])\n",
    "        x = self.dropout(self.relu(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(self.relu(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(self.X, pd.DataFrame):\n",
    "            X_tensor = torch.tensor(self.X.iloc[idx], dtype=torch.float32)        \n",
    "        else:   \n",
    "            X_tensor = torch.tensor(self.X[idx], dtype=torch.float32)        \n",
    "        X_tensor = X_tensor.unsqueeze(0)\n",
    "        \n",
    "        if self.y is not None: \n",
    "            if isinstance(self.y, pd.DataFrame):\n",
    "                y_tensor = torch.tensor(self.y.iloc[idx], dtype=torch.float32)\n",
    "            else:\n",
    "                y_tensor = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "            return X_tensor, y_tensor\n",
    "        return X_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedMSELoss(torch.nn.Module):\n",
    "    def __init__(self, scale=1.0):\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        residuals = labels - preds\n",
    "        weights = torch.where(residuals > 0, self.scale, 1.0)\n",
    "        loss = torch.mean(weights * residuals ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(X_train, X_valid, X_test, encode_col, target_col, smooth=0.0, agg=\"mean\"):\n",
    "    encoded_col = f'TE_{agg.upper()}_' + '_'.join(encode_col)\n",
    "    \n",
    "    df_tmp = X_train[encode_col + [target_col]].groupby(encode_col).agg([agg, 'count']).reset_index()\n",
    "    if agg==\"mean\": mn = X_train[target_col].mean()\n",
    "    elif agg==\"median\": mn = X_train[target_col].median()\n",
    "    elif agg==\"std\": mn = X_train[target_col].std()\n",
    "    elif agg==\"min\": mn = X_train[target_col].min()\n",
    "    elif agg==\"max\": mn = X_train[target_col].max()\n",
    "    \n",
    "    df_tmp.columns = encode_col + [agg, 'count']\n",
    "    df_tmp['TE_tmp'] = ((df_tmp[agg] * df_tmp['count']) + (mn * smooth)) / (df_tmp['count'] + smooth)\n",
    "    \n",
    "    X_train = X_train.merge(df_tmp[encode_col + ['TE_tmp']], how='left', left_on=encode_col, right_on=encode_col)\n",
    "    X_train[encoded_col] = X_train['TE_tmp'].fillna(mn)\n",
    "    X_train = X_train.drop(columns=['TE_tmp'])\n",
    "    # X_train[encoded_col] = X_train[encoded_col].astype(\"float32\")\n",
    "    \n",
    "    df_tmp_m = X_valid[encode_col].merge(df_tmp, how='left', left_on=encode_col, right_on=encode_col)\n",
    "    X_valid[encoded_col] = df_tmp_m['TE_tmp'].fillna(mn).values\n",
    "    # X_valid[encoded_col] = X_valid[encoded_col].astype(\"float32\")\n",
    "\n",
    "    df_tmp_m = X_test[encode_col].merge(df_tmp, how='left', left_on=encode_col, right_on=encode_col)\n",
    "    X_test[encoded_col] = df_tmp_m['TE_tmp'].fillna(mn).values\n",
    "    # X_test[encoded_col] = X_test[encoded_col].astype(\"float32\")\n",
    "    \n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train, train[['Price']]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=SEED)\n",
    "X_test = test\n",
    "\n",
    "X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "X_valid, y_valid = X_valid.reset_index(drop=True), y_valid.reset_index(drop=True)\n",
    "\n",
    "## Target Encoder\n",
    "encoder_columns = [\n",
    "    'Manufacturer', 'Model', \n",
    "    # ['Manufacturer', 'BatteryCapacity_cut'], \n",
    "    # ['Manufacturer', 'AccidentHistory'], \n",
    "    # ['Model', 'BatteryCapacity_cut'], \n",
    "    # ['Model', 'AccidentHistory'], \n",
    "]\n",
    "for column in encoder_columns:\n",
    "    if not isinstance(column, list): column = [column]\n",
    "    X_train, X_valid, X_test = target_encode(X_train, X_valid, X_test, encode_col=column, target_col='Price', smooth=0.0, agg=\"mean\")\n",
    "\n",
    "X_train = X_train.drop(columns=['Price'] + ['Manufacturer', 'Model'], axis=1) # \n",
    "X_valid = X_valid.drop(columns=['Price'] + ['Manufacturer', 'Model'], axis=1) # \n",
    "X_test = X_test.drop(columns=[] + ['Manufacturer', 'Model'], axis=1) # \n",
    "\n",
    "encoder_columns = ['VehicleCondition', 'DriveType', 'AccidentHistory'] # , 'BatteryCapacity_cut'\n",
    "encoder = ce.PolynomialEncoder(cols=encoder_columns)\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "X_valid = encoder.transform(X_valid)\n",
    "X_test = encoder.transform(X_test)\n",
    "\n",
    "scaler_columns = numerical_columns\n",
    "X_scaler = StandardScaler()\n",
    "X_train[scaler_columns] = X_scaler.fit_transform(X_train[scaler_columns])\n",
    "X_valid[scaler_columns] = X_scaler.transform(X_valid[scaler_columns])\n",
    "X_test[scaler_columns] = X_scaler.transform(X_test[scaler_columns])\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train[['Price']] = y_scaler.fit_transform(y_train[['Price']])\n",
    "y_valid[['Price']] = y_scaler.transform(y_valid[['Price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "patience = 50\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6747, 22)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "valid_dataset = CustomDataset(X_valid, y_valid)\n",
    "test_dataset = CustomDataset(X_test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleGRU(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "criterion = WeightedMSELoss(1.2) # torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, threshold=1e-12, min_lr=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:10<00:00,  5.26batch/s]\n",
      "Valid: 100%|██████████| 6/6 [00:00<00:00, 11.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 181/1000] Train Loss: 0.00349684 | Valid Loss: 0.00137491 | Learnin Rate : 1.953125e-06 | No improvement in validation loss. 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:09<00:00,  5.81batch/s]\n",
      "Valid: 100%|██████████| 6/6 [00:00<00:00, 13.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 182/1000] Train Loss: 0.00354254 | Valid Loss: 0.00136294 | Learnin Rate : 1.953125e-06 | No improvement in validation loss. 47/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:09<00:00,  5.85batch/s]\n",
      "Valid: 100%|██████████| 6/6 [00:00<00:00, 11.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 183/1000] Train Loss: 0.00341971 | Valid Loss: 0.00140964 | Learnin Rate : 1.953125e-06 | No improvement in validation loss. 48/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:09<00:00,  5.86batch/s]\n",
      "Valid: 100%|██████████| 6/6 [00:00<00:00,  9.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 184/1000] Train Loss: 0.00354384 | Valid Loss: 0.00140778 | Learnin Rate : 1.953125e-06 | No improvement in validation loss. 49/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:08<00:00,  6.05batch/s]\n",
      "Valid: 100%|██████████| 6/6 [00:00<00:00, 12.18batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 185/1000] Train Loss: 0.00359621 | Valid Loss: 0.00139146 | Learnin Rate : 1.953125e-06 | No improvement in validation loss. 50/50\n",
      "Early stopping triggered. Best valdation loss is 0.0013573304361974199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf') \n",
    "early_stop_counter = 0 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0    \n",
    "    train_pbar = tqdm(train_dataloader, unit='batch', desc='Train')\n",
    "    for _, (X_batch, y_batch) in enumerate(train_pbar):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)         \n",
    "        outputs = outputs\n",
    "        \n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_pbar = tqdm(valid_dataloader, unit='batch', desc='Valid')\n",
    "    with torch.no_grad():\n",
    "        for _, (X_batch, y_batch) in enumerate(valid_pbar):\n",
    "            val_outputs = model(X_batch)\n",
    "            val_outputs = val_outputs\n",
    "            \n",
    "            val_loss = criterion(val_outputs, y_batch)\n",
    "            valid_loss += val_loss.item()\n",
    "\n",
    "    valid_loss /= len(valid_dataloader)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {train_loss:.8f} | Valid Loss: {valid_loss:.8f}\", end=\" | \")\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step(valid_loss)\n",
    "        lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "        print(f\"Learnin Rate : {lr}\", end=\" | \")\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        early_stop_counter = 0 \n",
    "        best_model_state = model.state_dict() \n",
    "        torch.save(best_model_state, f'{PATH}/result/lstm/weights/best.pt')\n",
    "        print(f\"Validation loss improved\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"No improvement in validation loss. {early_stop_counter}/{patience}\")\n",
    "    \n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"Early stopping triggered. Best valdation loss is {best_valid_loss}\")\n",
    "        break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kgw\\AppData\\Local\\Temp\\ipykernel_18552\\4237052713.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  X_tensor = torch.tensor(self.X.iloc[idx], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_dataloader:\n",
    "        pred = model(X_batch)\n",
    "        pred = pred\n",
    "        predictions.append(pred.cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kgw\\AppData\\Local\\Temp\\ipykernel_18552\\2329340293.py:2: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  submit['가격(백만원)'] = list(map(float, y_scaler.inverse_transform(predictions))) # y_scaler.inverse_transform(predictions)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>가격(백만원)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>130.929901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>80.123482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>64.982758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>35.278687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>48.143429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID     가격(백만원)\n",
       "0  TEST_000  130.929901\n",
       "1  TEST_001   80.123482\n",
       "2  TEST_002   64.982758\n",
       "3  TEST_003   35.278687\n",
       "4  TEST_004   48.143429"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.read_csv(f'{PATH}/data/sample_submission.csv')\n",
    "submit['가격(백만원)'] = list(map(float, y_scaler.inverse_transform(predictions))) # y_scaler.inverse_transform(predictions)\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(f'{PATH}/result/lstm/LSTM_CV-{best_valid_loss:.12f}_LB-.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
