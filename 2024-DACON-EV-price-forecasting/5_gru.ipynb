{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "\n",
    "from warnings import simplefilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 909\n",
    "PATH = os.getcwd()\n",
    "train = pd.read_csv(f'{PATH}/data/train.csv')\n",
    "test = pd.read_csv(f'{PATH}/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_columns = {\n",
    "    \"제조사\": \"Manufacturer\",           \"모델\": \"Model\",\n",
    "    \"차량상태\": \"VehicleCondition\",     \"배터리용량\": \"BatteryCapacity\",\n",
    "    \"구동방식\": \"DriveType\",            \"주행거리(km)\": \"MileageKm\",\n",
    "    \"보증기간(년)\": \"WarrantyYears\",    \"사고이력\": \"AccidentHistory\",\n",
    "    \"연식(년)\": \"Year\",                 \"가격(백만원)\": \"Price\",\n",
    "}\n",
    "\n",
    "train = train.rename(columns=rename_columns)\n",
    "test = test.rename(columns=rename_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['BatteryCapacity'] = train['BatteryCapacity'].fillna(train.groupby(['Manufacturer', 'Model'])['BatteryCapacity'].transform('mean'))\n",
    "test['BatteryCapacity'] = test['BatteryCapacity'].fillna(test.groupby(['Manufacturer', 'Model'])['BatteryCapacity'].transform('mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_columns = ['ID', 'Price'] # , 'AccidentHistory', 'DriveType'\n",
    "categorical_columns = [col for col in train.columns if (train[col].dtype in ['object', 'category']) and (col not in without_columns)]\n",
    "numerical_columns  = [col for col in train.columns if col not in categorical_columns and (col not in without_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL ENCODE -> categorical features\n",
    "for feat in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    train[feat] = le.fit_transform(train[feat])\n",
    "    test[feat] = le.transform(test[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "target_encode_columns = []\n",
    "target_encode_columns = categorical_columns.copy() # + numerical_columns.copy()\n",
    "target_encode_columns += list(map(list, combinations(categorical_columns, 2)))\n",
    "target_encode_columns += list(map(list, combinations(categorical_columns, 3)))\n",
    "target_encode_columns += list(map(list, combinations(categorical_columns, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/rapidsai/deeplearning/blob/main/RecSys2020Tutorial/03_3_TargetEncoding.ipynb\n",
    "def target_encode(train, valid, test, encode_col, target_col, smooth=0.0, agg=\"mean\"):\n",
    "    encoded_col = f'TE_{agg.upper()}_' + '_'.join(encode_col)\n",
    "    \n",
    "    df_tmp = train[encode_col + [target_col]].groupby(encode_col).agg([agg, 'count']).reset_index()\n",
    "    if agg==\"mean\": mn = train[target_col].mean()\n",
    "    elif agg==\"median\": mn = train[target_col].median()\n",
    "    elif agg==\"std\": mn = train[target_col].std()\n",
    "    elif agg==\"min\": mn = train[target_col].min()\n",
    "    elif agg==\"max\": mn = train[target_col].max()\n",
    "    \n",
    "    df_tmp.columns = encode_col + [agg, 'count']\n",
    "    df_tmp['TE_tmp'] = ((df_tmp[agg] * df_tmp['count']) + (mn * smooth)) / (df_tmp['count'] + smooth)\n",
    "    \n",
    "    train = train.merge(df_tmp[encode_col + ['TE_tmp']], how='left', left_on=encode_col, right_on=encode_col)\n",
    "    train[encoded_col] = train['TE_tmp'].fillna(mn)\n",
    "    train = train.drop(columns=['TE_tmp'])\n",
    "    \n",
    "    df_tmp_m = valid[encode_col].merge(df_tmp, how='left', left_on=encode_col, right_on=encode_col)\n",
    "    valid[encoded_col] = df_tmp_m['TE_tmp'].fillna(mn).values\n",
    "\n",
    "    df_tmp_m = test[encode_col].merge(df_tmp, how='left', left_on=encode_col, right_on=encode_col)\n",
    "    test[encoded_col] = df_tmp_m['TE_tmp'].fillna(mn).values\n",
    "    \n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGRU(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.GRU = torch.nn.GRU(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size//2, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        x, _ = self.GRU(x, h0)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X: pd.DataFrame, y: pd.DataFrame=None):\n",
    "        self.X = X.reset_index(drop=True)\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X_values = self.X.iloc[idx].values\n",
    "\n",
    "        X_tensor = torch.tensor(X_values, dtype=torch.float32)\n",
    "        if self.y is not None: y_tensor = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "\n",
    "        # LSTM 입력 형식 (batch_size, seq_len, input_size)\n",
    "        X_tensor = X_tensor.unsqueeze(0)\n",
    "        \n",
    "        if self.y is not None:\n",
    "            return X_tensor, y_tensor   \n",
    "        else:\n",
    "            return X_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('ID', axis=1)\n",
    "X_test = test.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = train_test_split(X, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, feat in enumerate(target_encode_columns):                    \n",
    "    if isinstance(feat, list): c = feat\n",
    "    else: c = [feat]    \n",
    "    # TARGET ENCODE \n",
    "    X_train, X_valid, X_test = target_encode(X_train, X_valid, X_test, encode_col=c, target_col=\"Price\", smooth=0.0, agg=\"mean\")\n",
    "    X_train, X_valid, X_test = target_encode(X_train, X_valid, X_test, encode_col=c, target_col=\"Price\", smooth=0.0, agg=\"median\")\n",
    "    X_train, X_valid, X_test = target_encode(X_train, X_valid, X_test, encode_col=c, target_col=\"Price\", smooth=0.0, agg=\"std\")\n",
    "    \n",
    "X_train, y_train = X_train.drop('Price', axis=1), X_train[['Price']]\n",
    "X_valid, y_valid = X_valid.drop('Price', axis=1), X_valid[['Price']]\n",
    "            \n",
    "X_scaler = StandardScaler()\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "X_valid_scaled = X_scaler.transform(X_valid)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_valid_scaled = y_scaler.transform(y_valid)\n",
    "\n",
    "X_train_reconstruct = pd.DataFrame(data=X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_valid_reconstruct = pd.DataFrame(data=X_valid_scaled, columns=X_valid.columns, index=X_valid.index)\n",
    "test_reconstruct = pd.DataFrame(data=X_test_scaled, columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(X_train_reconstruct, y_train_scaled)\n",
    "valid_dataset = CustomDataset(X_valid_reconstruct, y_valid_scaled)\n",
    "test_dataset = CustomDataset(test_reconstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "batch_size = 1024\n",
    "patience = 30\n",
    "\n",
    "input_size = X_train_reconstruct.shape[1]\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleGRU(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, threshold=1e-10, min_lr=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 6/6 [00:01<00:00,  3.61batch/s]\n",
      "Valid: 100%|██████████| 2/2 [00:00<00:00,  7.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 931/1000] Train Loss: 0.00185527 | Valid Loss: 0.00230521 | Learnin Rate : 6.103515625e-07 | No improvement in validation loss. 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 6/6 [00:02<00:00,  2.28batch/s]\n",
      "Valid: 100%|██████████| 2/2 [00:00<00:00,  7.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 932/1000] Train Loss: 0.00186034 | Valid Loss: 0.00230518 | Learnin Rate : 6.103515625e-07 | No improvement in validation loss. 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 6/6 [00:01<00:00,  3.71batch/s]\n",
      "Valid: 100%|██████████| 2/2 [00:00<00:00,  9.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 933/1000] Train Loss: 0.00186279 | Valid Loss: 0.00230521 | Learnin Rate : 6.103515625e-07 | No improvement in validation loss. 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 6/6 [00:01<00:00,  4.24batch/s]\n",
      "Valid: 100%|██████████| 2/2 [00:00<00:00,  9.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 934/1000] Train Loss: 0.00186776 | Valid Loss: 0.00230519 | Learnin Rate : 6.103515625e-07 | No improvement in validation loss. 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 6/6 [00:01<00:00,  4.19batch/s]\n",
      "Valid: 100%|██████████| 2/2 [00:00<00:00,  9.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 935/1000] Train Loss: 0.00186736 | Valid Loss: 0.00230524 | Learnin Rate : 6.103515625e-07 | No improvement in validation loss. 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 6/6 [00:01<00:00,  4.28batch/s]\n",
      "Valid: 100%|██████████| 2/2 [00:00<00:00,  7.75batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 936/1000] Train Loss: 0.00186065 | Valid Loss: 0.00230518 | Learnin Rate : 6.103515625e-07 | No improvement in validation loss. 30/30\n",
      "Early stopping triggered. Best valdation loss is 0.002305082860402763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf') \n",
    "early_stop_counter = 0 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0    \n",
    "    train_pbar = tqdm(train_dataloader, unit='batch', desc='Train')\n",
    "    for _, (X_batch, y_batch) in enumerate(train_pbar):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch) \n",
    "        \n",
    "        outputs = outputs[:, -1, :] \n",
    "        \n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_pbar = tqdm(valid_dataloader, unit='batch', desc='Valid')\n",
    "    with torch.no_grad():\n",
    "        for _, (X_val, y_val) in enumerate(valid_pbar):\n",
    "            val_outputs = model(X_val)\n",
    "            val_outputs = val_outputs[:, -1, :] \n",
    "            \n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "            valid_loss += val_loss.item()\n",
    "\n",
    "    valid_loss /= len(valid_dataloader)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {train_loss:.8f} | Valid Loss: {valid_loss:.8f}\", end=\" | \")\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step(valid_loss)\n",
    "        lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "        print(f\"Learnin Rate : {lr}\", end=\" | \")\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        early_stop_counter = 0 \n",
    "        best_model_state = model.state_dict() \n",
    "        print(f\"Validation loss improved\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"No improvement in validation loss. {early_stop_counter}/{patience}\")\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"Early stopping triggered. Best valdation loss is {best_valid_loss}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_dataloader:\n",
    "        pred = model(X_batch)\n",
    "        pred = pred[:, -1, :]  # (batch_size, 1)\n",
    "        predictions.append(pred.cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)  # (전체 검증 샘플 수, 1)\n",
    "\n",
    "# 스케일러 역변환\n",
    "predictions_inversed = y_scaler.inverse_transform(predictions)  # 원본 값으로 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>가격(백만원)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>130.792572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>79.693657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>55.093788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>34.899097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>45.692009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID     가격(백만원)\n",
       "0  TEST_000  130.792572\n",
       "1  TEST_001   79.693657\n",
       "2  TEST_002   55.093788\n",
       "3  TEST_003   34.899097\n",
       "4  TEST_004   45.692009"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.read_csv(f'{PATH}/data/sample_submission.csv')\n",
    "submit['가격(백만원)'] = list(map(float, predictions_inversed.flatten()))\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(f'{PATH}/result/lstm/LSTM_CV-{best_valid_loss:.6f}_LB-.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
